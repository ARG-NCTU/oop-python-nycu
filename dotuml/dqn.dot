ClassDiagram{
    class nnModule{}
// common==============================================================================================
    class BaseModel{
        __init__(observation_space: spaces.Space,
        action_space: spaces.Space,
        features_extractor_class: FlattenExtractor,
        features_extractor: BaseFeaturesExtractor)
        _update_features_extractor(): Dict
        make_features_extractor(): BaseFeaturesExtractor
        extract_features(): th.Tensor
        _get_constructor_parameters(): Dict

        save(): None
        load(): SelfBaseModel
    }
    //
    class BasePolicy{
        features_extractor: BaseFeaturesExtractor
        __init__()
        init_weights(module: nn.Module, gain: float): None
        _predict(observation: th.Tensor, deterministic: bool): th.Tensor
        scale_action(action: np.ndarray): np.ndarray
        unscale_action(scled_action: np.ndarray): np.ndarray
    }
    class BaseFeaturesExtractor{
        __init__(bservation_space: gym.Space, features_dim: int): None
    }
    class FlattenExtractor{
        __init__(observation_space: gym.Space): None
        forward(self, observations: th.Tensor): th.Tensor
    }
    class NatureCNN{
        __init__(observation_space: gym.Space): None
        forward(self, observations: th.Tensor): th.Tensor
    }
    class BaseAlgorithm{
        __init__(policy: BasePolicy, env: GymEnv): None
    }
    class offPolicyAlorithm{
        __init__(policy: BasePolicy, env: GymEnv): None
        save_replay_buffer(path): None
        load_replay_buffer(path): None
        learn(): SelfOffPolicyAlgorithm
        train(): None
        collect_rollouts(env: VecEnv,
                        callback: BaseCallback,
                        train_freq: TrainFreq,
                        replay_buffer: ReplayBuffer): RolloutBuffer
    }
//=====================================================================================================
//====================================policy===========================================================
    class QNetwork{
        __init__(observation_space: spaces.Space, 
                action_space: spaces.Discrete, 
                features_extractor: BaseFeaturesExtractor, 
                activation_fn: nn.Module)
        
        forward(observation: torchTensor, deterministic: bool): torch.Tensor

        _predict(observation: torchTensor): torch.Tensor

        _get_constructor_parameters(): Dict
    }

    class DQNPolicy{
        q_net: QNetwork
        q_net_target: QNetwork

        __init__(observation_space: spaces.Space,
                action_space: spaces.Discrete,
                lr_schedule: Schedule,
                net_arch: Optional,
                activation_fn: nnModule,
                features_extractor_class: FlattenExtractor,
                features_extractor_kwargs: Dict,
                optimizer_class: th.optim.Opt_imizer,
                optimizer_kwargs: Dict)

        _build(lr_schedule: Schedule): None
        make_q_net(): QNetwork
        forward(obs: th.Tensor): th.Tensor
        _predict(obs: th.Tensor): th.Tensor
    }

    class MlpPolicy{}

    class CnnPolicy {
        __init__(observation_space: spaces.Space, 
                action_space: spaces.Discrete, 
                lr_schedule: Schedule,
                activation_fn: nn.Module,
                features_extractor_class: NatureCNN,
                optimizer_class: th.optim.Adam)
    }

    class MultiInputPolicy{
        __init__(observation_space: spaces.Dict,
                action_space: spaces.Discrete,
                lr_schedule: Schedule,
                activation_fn: nn.Module,
                features_extractor_class: CombinedExtractor,
                optimizer_class: th.optim.Adam)
    }
//===========================================================================================
//========================================dqn================================================
    class DQN{
        MlpPolicy: MlpPolicy
        CnnPolicy: CnnPolicy
        MultiInputPolicy: MultiInputPolicy
        exploration_schedule: Schedule
        q_net: QNetwork
        q_net_target: QNetwork
        policy: DQNPolicy

        __init__(policy: DQNPolicy,
        env: GymEnv,
        learning_rate: Schedule,
        replay_buffer_class: ReplayBuffer,
        device: th.device)

        train(): None
        predict(observation: np.ndarray,
                episode_start: np.ndarray): Tuple
        learn(self: SelfDQN): SelfDQN

        _excluded_save_params(): List
        _get_torch_save_params(): Tuple

    }
//===========================================================================================

// a: Aggregation, c: Composition, g: inherit
    BaseModel -g-> nnModule;
    BasePolicy -g-> BaseModel;
    BaseFeaturesExtractor -g-> nnModule;
    FlattenExtractor -g-> BaseFeaturesExtractor;
    NatureCNN -g-> BaseFeaturesExtractor;
    offPolicyAlorithm -g-> BaseAlgorithm;

    BaseAlgorithm --> "0..*" BasePolicy;
    BaseFeaturesExtractor --> "0..*" BasePolicy;
    BaseFeaturesExtractor --> "0..*" BaseModel;
    FlattenExtractor --> "0..*" BaseModel;
    //
    QNetwork -g-> BasePolicy;
    DQNPolicy -g-> QNetwork;
    MlpPolicy -g-> DQNPolicy;
    CnnPolicy -g-> DQNPolicy;
    MultiInputPolicy -g-> DQNPolicy;
    //
    QNetwork --> "0..*" DQNPolicy;
    nnModule --> "0..*" DQNPolicy;
    FlattenExtractor --> "0..*" DQNPolicy;
    NatureCNN --> "0..*" CnnPolicy;
    //
    DQN -g-> offPolicyAlorithm;

    CnnPolicy --> "0..*" DQN;
    MlpPolicy --> "0..*" DQN;
    MultiInputPolicy --> "0..*" DQN;
    DQNPolicy --> "0..*" DQN;
    QNetwork --> "0..*" DQN;
}